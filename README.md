This project implements a Retrieval-Augmented Generation (RAG) based Knowledge Q&A Chatbot that answers user questions using information extracted from PDF documents. The system combines semantic search with a local Large Language Model (LLM) running via Ollama, ensuring accurate, context-aware, and hallucination-free responses.

The chatbot ingests domain-specific documents (Machine Learning PDFs), stores them in a vector database (ChromaDB), retrieves the most relevant content using embeddings, and generates answers grounded strictly in the retrieved context.


ğŸš€ Key Features :

ğŸ“„ PDF-based knowledge ingestion
ğŸ” Semantic search using vector embeddings
ğŸ§  Retrieval-Augmented Generation (RAG) architecture
ğŸ¤– Local LLM inference using Ollama (LLaMA 3.2 â€“ 1B)
ğŸ—‚ï¸ Vector database with ChromaDB
âš¡ FastAPI-based backend (optional API mode)
ğŸ§  In-memory conversational caching
ğŸ§ª Unit testing with pytest
ğŸ” Offline, cost-free, and privacy-friendly

ğŸ› ï¸ Tech Stack :

Language: Python
LLM: Ollama (LLaMA 3.2 â€“ 1B)
Framework: LangChain
Vector DB: ChromaDB
Embeddings: HuggingFace Sentence Transformers
API: FastAPI + Uvicorn
Testing: pytest

ğŸ—ï¸ Architecture :

1.) The system follows a RAG pipeline:
2.) Document ingestion and chunking
3.) Embedding generation
4.) Vector storage
5.) Query-time retrieval
6.) Context-aware response generation using LLM
